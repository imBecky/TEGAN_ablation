{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8LAdiTaSKgc5"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.io import loadmat\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "L3Nfop2_LH1x"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import tensorflow as tf\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "CLASSES_NUM = 6  # 输出7类地物\n",
        "LABELS = ['', 'Trees', 'Asphalt', 'Parking lot', 'Bitumen', 'Meadow', 'Soil']\n",
        "VAL_FRAC = 0.5\n",
        "TEST_FRAC = 0.3  # target用来测试数据的百分比 test/train\n",
        "TRAIN_FRAC = 0.7\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 50\n",
        "BANDS = 72\n",
        "EPOCHS = 1\n",
        "PATIENCE = 15\n",
        "noise_dim = 72\n",
        "num_examples_to_generate = 16\n",
        "# seed = tf.random.normal([BATCH_SIZE, 72, 1])\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "FEATURE_dim = 36\n",
        "lr = 4e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BbVy3bs5LZ6q"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue May 14 12:16:11 2019\n",
        "@author: viryl\n",
        "\"\"\"\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from param import *\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "def gen_dataset_from_dict(file_dict, Val=False):\n",
        "    data = file_dict['data']\n",
        "    data = np.transpose(data, (0, 2, 1))\n",
        "    label = file_dict['gt']\n",
        "    data_train, data_test, label_train, label_test = train_test_split(data, label, test_size=TEST_FRAC, random_state=42)\n",
        "    if Val:\n",
        "        data_test, data_val, label_test, label_val = train_test_split(data_test, label_test, test_size=VAL_FRAC,\n",
        "                                                                      random_state=43)\n",
        "    data_train = tf.data.Dataset.from_tensor_slices(data_train)\n",
        "    data_test = tf.data.Dataset.from_tensor_slices(data_test)\n",
        "    label_train = tf.data.Dataset.from_tensor_slices(label_train)\n",
        "    label_test = tf.data.Dataset.from_tensor_slices(label_test)\n",
        "    if Val:\n",
        "        data_val = tf.data.Dataset.from_tensor_slices(data_val)\n",
        "        label_val = tf.data.Dataset.from_tensor_slices(label_val)\n",
        "        val_ds = tf.data.Dataset.zip((data_val, label_val))\n",
        "        val_ds = val_ds.map(lambda x, y: {'data': x, 'label': y}).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "    train_ds = tf.data.Dataset.zip((data_train, label_train))\n",
        "    test_ds = tf.data.Dataset.zip((data_test, label_test))\n",
        "\n",
        "    train_ds = train_ds.map(lambda x, y: {'data': x, 'label': y}).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    test_ds = test_ds.map(lambda x, y: {'data': x, 'label': y}).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    if Val:\n",
        "        return train_ds, test_ds, val_ds\n",
        "    else:\n",
        "        return train_ds, test_ds\n",
        "\n",
        "\n",
        "def generate_and_save_Images(model, epoch, test_input):\n",
        "    \"\"\"Notice `training` is set to False.\n",
        "       This is so all layers run in inference mode (batch norm).\"\"\"\n",
        "    \"\"\"To-do: reshape the curves as they were normalized\"\"\"\n",
        "    prediction = model(test_input, training=False)\n",
        "    plt.plot(np.arange(72), prediction[0, :, 0])\n",
        "    plt.savefig('./pics/image_at_{:04d}_epoch.png'.format(epoch))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_data_from_batch(batches):\n",
        "    return batches['data'], batches['label']\n",
        "\n",
        "\n",
        "def calculate_acc(target_test_ds,\n",
        "                  classifier,\n",
        "                  epoch):\n",
        "    target_batch = target_test_ds.shuffle(BUFFER_SIZE).as_numpy_iterator().next()\n",
        "    target_data, target_label = get_data_from_batch(target_batch)\n",
        "    prediction_t = classifier(target_data, training=False)\n",
        "    accuracy_t = tf.metrics.Accuracy()\n",
        "    accuracy_t.update_state(y_true=target_label,\n",
        "                            y_pred=prediction_t)\n",
        "    print('Target accuracy for epoch {} is'.format(epoch + 1),\n",
        "          '{}%'.format(accuracy_t.result().numpy() * 100))\n",
        "\n",
        "\n",
        "def plot_acc_loss(acc, gen_loss, disc_loss, cls_loss,\n",
        "                  generator_loss, discriminator_loss, classifier_loss,\n",
        "                  source_test_ds, target_test_ds,\n",
        "                  generator, discriminator, classifier,\n",
        "                  epoch):\n",
        "    g_loss, d_loss, c_loss, a = [], [], [], []\n",
        "    for source_test_batch in source_test_ds.as_numpy_iterator():\n",
        "        for target_test_batch in target_test_ds.as_numpy_iterator():\n",
        "            X_s, Y_s = get_data_from_batch(source_test_batch)\n",
        "            X_t, Y_t = get_data_from_batch(target_test_batch)\n",
        "            generated_target = generator(X_s, training=False)\n",
        "            real_decision = discriminator(X_t, training=False)\n",
        "            fake_decision = discriminator(generated_target, training=False)\n",
        "            prediction = classifier(X_t, training=False)\n",
        "            accuracy_t = tf.metrics.Accuracy()\n",
        "            accuracy_t.update_state(y_true=Y_t,\n",
        "                                    y_pred=prediction)\n",
        "            a.append(accuracy_t.result().numpy())\n",
        "            c_loss.append(classifier_loss(prediction, Y_t).numpy())\n",
        "            g_loss.append(generator_loss(fake_decision).numpy())\n",
        "            d_loss.append(discriminator_loss(real_decision, fake_decision).numpy())\n",
        "    a = np.average(a)\n",
        "    acc.append(a)\n",
        "    cls_loss.append(np.average(c_loss))\n",
        "    gen_loss.append(np.average(g_loss))\n",
        "    disc_loss.append(np.average(d_loss))\n",
        "    epochs_range = range(epoch+1)\n",
        "    print(epochs_range)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, gen_loss, label='Generator_loss')\n",
        "    plt.plot(epochs_range, disc_loss, label='Discriminator_loss')\n",
        "    plt.plot(epochs_range, cls_loss, label='Classifier_loss')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Generator and discriminator loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, acc, label='Test accuracy')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "    return acc, gen_loss, disc_loss, cls_loss\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue May 14 12:16:11 2019\n",
        "@author: viryl\n",
        "\"\"\"\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from param import *\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "def gen_dataset_from_dict(file_dict, Val=False):\n",
        "    data = file_dict['data']\n",
        "    data = np.transpose(data, (0, 2, 1))\n",
        "    label = file_dict['gt']\n",
        "    data_train, data_test, label_train, label_test = train_test_split(data, label, test_size=TEST_FRAC, random_state=42)\n",
        "    if Val:\n",
        "        data_test, data_val, label_test, label_val = train_test_split(data_test, label_test, test_size=VAL_FRAC,\n",
        "                                                                      random_state=43)\n",
        "    data_train = tf.data.Dataset.from_tensor_slices(data_train)\n",
        "    data_test = tf.data.Dataset.from_tensor_slices(data_test)\n",
        "    label_train = tf.data.Dataset.from_tensor_slices(label_train)\n",
        "    label_test = tf.data.Dataset.from_tensor_slices(label_test)\n",
        "    if Val:\n",
        "        data_val = tf.data.Dataset.from_tensor_slices(data_val)\n",
        "        label_val = tf.data.Dataset.from_tensor_slices(label_val)\n",
        "        val_ds = tf.data.Dataset.zip((data_val, label_val))\n",
        "        val_ds = val_ds.map(lambda x, y: {'data': x, 'label': y}).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "    train_ds = tf.data.Dataset.zip((data_train, label_train))\n",
        "    test_ds = tf.data.Dataset.zip((data_test, label_test))\n",
        "\n",
        "    train_ds = train_ds.map(lambda x, y: {'data': x, 'label': y}).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    test_ds = test_ds.map(lambda x, y: {'data': x, 'label': y}).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    if Val:\n",
        "        return train_ds, test_ds, val_ds\n",
        "    else:\n",
        "        return train_ds, test_ds\n",
        "\n",
        "\n",
        "def generate_and_save_Images(model, epoch, test_input):\n",
        "    \"\"\"Notice `training` is set to False.\n",
        "       This is so all layers run in inference mode (batch norm).\"\"\"\n",
        "    \"\"\"To-do: reshape the curves as they were normalized\"\"\"\n",
        "    prediction = model(test_input, training=False)\n",
        "    plt.plot(np.arange(72), prediction[0, :, 0])\n",
        "    plt.savefig('./pics/image_at_{:04d}_epoch.png'.format(epoch))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_data_from_batch(batches):\n",
        "    return batches['data'], batches['label']\n",
        "\n",
        "\n",
        "def calculate_acc(target_test_ds,\n",
        "                  classifier,\n",
        "                  epoch):\n",
        "    target_batch = target_test_ds.shuffle(BUFFER_SIZE).as_numpy_iterator().next()\n",
        "    target_data, target_label = get_data_from_batch(target_batch)\n",
        "    prediction_t = classifier(target_data, training=False)\n",
        "    accuracy_t = tf.metrics.Accuracy()\n",
        "    accuracy_t.update_state(y_true=target_label,\n",
        "                            y_pred=prediction_t)\n",
        "    print('Target accuracy for epoch {} is'.format(epoch + 1),\n",
        "          '{}%'.format(accuracy_t.result().numpy() * 100))\n",
        "\n",
        "\n",
        "def plot_acc_loss(acc, gen_loss, disc_loss, cls_loss,\n",
        "                  generator_loss, discriminator_loss, classifier_loss,\n",
        "                  source_test_ds, target_test_ds,\n",
        "                  generator, discriminator, classifier,\n",
        "                  epoch):\n",
        "    g_loss, d_loss, c_loss, a = [], [], [], []\n",
        "    for source_test_batch in source_test_ds.as_numpy_iterator():\n",
        "        for target_test_batch in target_test_ds.as_numpy_iterator():\n",
        "            X_s, Y_s = get_data_from_batch(source_test_batch)\n",
        "            X_t, Y_t = get_data_from_batch(target_test_batch)\n",
        "            generated_target = generator(X_s, training=False)\n",
        "            real_decision = discriminator(X_t, training=False)\n",
        "            fake_decision = discriminator(generated_target, training=False)\n",
        "            prediction = classifier(X_t, training=False)\n",
        "            accuracy_t = tf.metrics.Accuracy()\n",
        "            accuracy_t.update_state(y_true=Y_t,\n",
        "                                    y_pred=prediction)\n",
        "            a.append(accuracy_t.result().numpy())\n",
        "            c_loss.append(classifier_loss(prediction, Y_t).numpy())\n",
        "            g_loss.append(generator_loss(fake_decision).numpy())\n",
        "            d_loss.append(discriminator_loss(real_decision, fake_decision).numpy())\n",
        "    a = np.average(a)\n",
        "    acc.append(a)\n",
        "    cls_loss.append(np.average(c_loss))\n",
        "    gen_loss.append(np.average(g_loss))\n",
        "    disc_loss.append(np.average(d_loss))\n",
        "    epochs_range = range(epoch+1)\n",
        "    print(epochs_range)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, gen_loss, label='Generator_loss')\n",
        "    plt.plot(epochs_range, disc_loss, label='Discriminator_loss')\n",
        "    plt.plot(epochs_range, cls_loss, label='Classifier_loss')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Generator and discriminator loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, acc, label='Test accuracy')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "    return acc, gen_loss, disc_loss, cls_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3nQghxgNLPOr"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "NEW_DATA_PATH = './new data'\n",
        "\"\"\"load data\"\"\"\n",
        "source_dict = sio.loadmat('/content/Source.mat')\n",
        "source_train_ds, source_test_ds = gen_dataset_from_dict(source_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "yATr-ER9ODhp",
        "outputId": "fe52b596-d6e2-41d6-9ff6-bde3153e6393"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "target_dict = sio.loadmat('/content/Target.mat')\n",
        "target_train_ds, target_test_ds, target_val_ds = gen_dataset_from_dict(target_dict, Val=True)\n",
        "\n",
        "\n",
        "plt.plot(np.arange(72), source_train_ds.as_numpy_iterator().next()['data'][0, :, 0])\n",
        "plt.title('Source')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muS4pO9k1bwI",
        "outputId": "dd952805-06c9-47a7-d199-1a8e7b759c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QcDMbbr-OlSL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "class ResBlock_up_top(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_channels):\n",
        "        super().__init__()\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        self.unsampling = layers.UpSampling1D()\n",
        "        self.conv = tfa.layers.SpectralNormalization(layers.Conv1D(filters=output_channels,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   strides=1,\n",
        "                                                                   padding='same',\n",
        "                                                                   use_bias=False))\n",
        "        self.conv_skip = tfa.layers.SpectralNormalization(layers.Conv1D(filters=output_channels,\n",
        "                                                                        kernel_size=1,\n",
        "                                                                        strides=1,\n",
        "                                                                        padding='same',\n",
        "                                                                        use_bias=False))\n",
        "        self.relu = layers.LeakyReLU()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        res = inputs\n",
        "        res = self.unsampling(res)\n",
        "        res = self.conv_skip(res)\n",
        "\n",
        "        x = self.bn(inputs)\n",
        "        x = self.relu(x)\n",
        "        x = self.unsampling(x)\n",
        "        x = self.conv_skip(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x + res\n",
        "\n",
        "\n",
        "class ResBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_channels):\n",
        "        super().__init__()\n",
        "        self.Conv = tfa.layers.SpectralNormalization(layers.Conv1D(filters=output_channels,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   strides=1,\n",
        "                                                                   padding='same',\n",
        "                                                                   use_bias=False))\n",
        "        self.Relu = layers.LeakyReLU()\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        res = inputs\n",
        "        x = self.Conv(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.Conv(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + res\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock_up(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_channels, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "        self.relu = layers.LeakyReLU()\n",
        "        self.unsampling = layers.UpSampling1D()\n",
        "        self.conv = tfa.layers.SpectralNormalization(layers.Conv1D(filters=output_channels,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   strides=1,\n",
        "                                                                   padding='same',\n",
        "                                                                   use_bias=False))\n",
        "        self.conv_skip = tfa.layers.SpectralNormalization(layers.Conv1D(filters=output_channels,\n",
        "                                                                        kernel_size=1,\n",
        "                                                                        padding='same',\n",
        "                                                                        use_bias=False))\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        res = inputs\n",
        "        res = self.unsampling(res)\n",
        "        res = self.conv_skip(res)\n",
        "        x = self.bn(inputs)\n",
        "        x = self.relu(x)\n",
        "        x = self.unsampling(x)\n",
        "        x = self.conv_skip(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.dropout(x)\n",
        "        return x + res\n",
        "\n",
        "\n",
        "class ResBlock_Down(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_channels):\n",
        "        super().__init__()\n",
        "        self.relu = layers.LeakyReLU()\n",
        "        self.conv = tfa.layers.SpectralNormalization(layers.Conv1D(filters=output_channels,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding='same',\n",
        "                                                                   use_bias=False))\n",
        "        self.conv_skip = tfa.layers.SpectralNormalization(layers.Conv1D(filters=output_channels,\n",
        "                                                                        kernel_size=1,\n",
        "                                                                        padding='same',\n",
        "                                                                        use_bias=False))\n",
        "        self.avg_pooling = layers.AveragePooling1D(padding='same')\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        res = self.conv_skip(inputs)\n",
        "        res = self.avg_pooling(res)\n",
        "\n",
        "        x = self.relu(inputs)\n",
        "        x = self.conv_skip(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.avg_pooling(x)\n",
        "\n",
        "        return x + res\n",
        "\n",
        "\n",
        "class Res_Dense(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.dense = tfa.layers.SpectralNormalization(layers.Dense(units))\n",
        "        self.drop = layers.Dropout(0.3)\n",
        "        self.relu = layers.LeakyReLU()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        res = inputs\n",
        "        res = self.dense(res)\n",
        "        res = self.relu(res)\n",
        "\n",
        "        x = self.dense(inputs)\n",
        "        x = self.drop(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x + res\n",
        "\n",
        "\n",
        "class bottle_neck(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_channels):\n",
        "        super().__init__()\n",
        "        self.filters = output_channels\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        self.conv = layers.Conv1D(filters=output_channels,\n",
        "                                  kernel_size=3,\n",
        "                                  padding='same',\n",
        "                                  use_bias=False)\n",
        "        self.conv_skip = layers.Conv1D(filters=output_channels,\n",
        "                                       kernel_size=1,\n",
        "                                       strides=1,\n",
        "                                       padding='same',\n",
        "                                       use_bias=False)\n",
        "        self.conv_trans = layers.Conv1D(filters=output_channels,\n",
        "                                                 kernel_size=1,\n",
        "                                                 strides=1,\n",
        "                                                 padding='same',\n",
        "                                                 use_bias=False)\n",
        "        self.unsampling = layers.UpSampling1D()\n",
        "        self.relu = layers.LeakyReLU()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        if self.filters != inputs.get_shape().as_list()[-1]:\n",
        "            res = self.conv_skip(inputs)\n",
        "        else:\n",
        "            res = inputs\n",
        "        x = self.conv(inputs)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_trans(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        z = x + res\n",
        "\n",
        "        return x + res\n",
        "\n",
        "\n",
        "class bottleneck_rev_s(layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.filters = channels\n",
        "        self.unit = bottle_neck(int(channels//2.0))\n",
        "        self.dense = layers.Dense(int(channels//2.0))\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x1, x2 = tf.split(inputs, 2, 2)\n",
        "        if self.filters != inputs.get_shape().as_list()[-1]:\n",
        "            x1 = self.dense(x1)\n",
        "        y1 = x1 + self.unit(x2)\n",
        "        y2 = x2\n",
        "        z = tf.concat([y2, y1], axis=2)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ResBlock_no_sn(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.dense1 = layers.Dense(units)\n",
        "        self.dense2 = layers.Dense(units)\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.relu = layers.LeakyReLU()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        res = self.dense1(inputs)\n",
        "        res = self.relu(res)\n",
        "\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        return x+res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "17JBCUE_O5Ih"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython import display\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "BANDS = 72\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(72, 1)))\n",
        "    model.add(ResBlock_up_top(72))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv1D(\n",
        "        filters=72,\n",
        "        kernel_size=3,\n",
        "        padding='same',\n",
        "        use_bias=False\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Reshape(target_shape=(72, 144)))\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "    model.add(tf.keras.layers.Dense(72, use_bias=True))\n",
        "    model.add(tf.keras.layers.Dense(1, use_bias=False))\n",
        "    model.build()\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(72, 1)))\n",
        "    model.add(ResBlock_Down(72))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(Res_Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_discriminator_domain_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=(FEATURE_dim, 1)))\n",
        "    model.add(ResBlock_Down(72))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(Res_Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_classifier_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(FEATURE_dim*2, input_shape=(36, 1)))\n",
        "    model.add(ResBlock_up_top(FEATURE_dim))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(ResBlock_up(FEATURE_dim))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(CLASSES_NUM, activation='relu'))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(CLASSES_NUM))\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_encoder_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(72, 1)))\n",
        "    model.add(layers.Conv1D(filters=72,\n",
        "                            kernel_size=7,\n",
        "                            strides=1,\n",
        "                            padding='same',\n",
        "                            use_bias=False))\n",
        "    model.add(layers.MaxPool1D())\n",
        "    model.add(bottleneck_rev_s(36))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "# define losses\n",
        "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "cat_cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = binary_cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = binary_cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return binary_cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "\n",
        "def classifier_loss(prediction, label):\n",
        "    return cat_cross_entropy(label, prediction)\n",
        "\n",
        "\n",
        "generator_s_optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "generator_t_optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "discriminator_t_optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "discriminator_s_optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "discriminator_domain_optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "encoder_s_optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "encoder_t_optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "classifier_optimizer = tf.keras.optimizers.Adagrad(lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "q8Q7mHb515V1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "domain_source_loss = tf.keras.metrics.Mean('domain_source_loss', dtype=tf.float32)\n",
        "domain_target_loss = tf.keras.metrics.Mean('domain_target_loss', dtype=tf.float32)\n",
        "gen_loss = tf.keras.metrics.Mean('generator_source_loss', dtype=tf.float32)\n",
        "disc_loss = tf.keras.metrics.Mean('discriminator_t_loss', dtype=tf.float32)\n",
        "source_test_accuracy = tf.keras.metrics.CategoricalAccuracy('source_test_accuracy')\n",
        "target_test_accuracy = tf.keras.metrics.CategoricalAccuracy('target_test_accuracy')\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "\n",
        "\n",
        "def test_step1(encoder_s, encoder_t, classifier, source_batch, target_batch):\n",
        "    Xs, Ys = get_data_from_batch(source_batch)\n",
        "    Xt, Yt = get_data_from_batch(target_batch)\n",
        "    feature_s = encoder_s(Xs, training=False)\n",
        "    feature_t = encoder_t(Xt, training=False)\n",
        "    prediction_s = classifier(feature_s, training=False)\n",
        "    prediction_t = classifier(feature_t, training=False)\n",
        "    loss = (classifier_loss(prediction_s, Ys).numpy() + classifier_loss(prediction_t, Yt).numpy()) / 2\n",
        "    test_loss(loss)\n",
        "    source_test_accuracy(Ys, prediction_s)\n",
        "    target_test_accuracy(Yt, prediction_t)\n",
        "\n",
        "\n",
        "def test_step2(encoder_s, encoder_t, discriminator_domain,\n",
        "               source_batch, target_batch):\n",
        "    Xs, Ys = get_data_from_batch(source_batch)\n",
        "    Xt, Yt = get_data_from_batch(target_batch)\n",
        "    feature_s = encoder_s(Xs, training=False)\n",
        "    feature_t = encoder_t(Xt, training=False)\n",
        "    source_decision = discriminator_domain(feature_s, training=False)\n",
        "    target_decision = discriminator_domain(feature_t, training=False)\n",
        "    domain_source_loss(discriminator_loss(source_decision, target_decision).numpy())\n",
        "    domain_target_loss(discriminator_loss(target_decision, source_decision).numpy())\n",
        "\n",
        "\n",
        "def test_step3(generator_s, generator_t, discriminator_t, discriminator_s,\n",
        "               source_batch, target_batch):\n",
        "    Xs, Ys = get_data_from_batch(source_batch)\n",
        "    Xt, Yt = get_data_from_batch(target_batch)\n",
        "    generated_t = generator_s(Xs, training=False)\n",
        "    real_target_output = discriminator_t(Xt, training=False)\n",
        "    fake_target_output = discriminator_t(generated_t, training=False)\n",
        "    gen_s_loss = generator_loss(fake_target_output)\n",
        "    disc_t_loss = discriminator_loss(real_target_output, fake_target_output)\n",
        "\n",
        "    gen_loss(gen_s_loss)\n",
        "    disc_loss(disc_t_loss)\n",
        "\n",
        "\n",
        "def test_step4(generator_s, encoder_t, classifier, source_batch):\n",
        "    Xs, Ys = get_data_from_batch(source_batch)\n",
        "    generated_t = generator_s(Xs, training=False)\n",
        "    feature_t = encoder_t(generated_t, training=False)\n",
        "    prediction = classifier(feature_t, training=False)\n",
        "    loss = classifier_loss(prediction, Ys)\n",
        "    test_loss(loss)\n",
        "    target_test_accuracy(Ys, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "L74nNzToO9ho"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "import scipy.io as sio\n",
        "import os\n",
        "generator_train_loss = tf.keras.metrics.Mean('generator_loss', dtype=tf.float32)\n",
        "discriminator_train_loss = tf.keras.metrics.Mean('discriminator_loss', dtype=tf.float32)\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "source_train_accuracy = tf.keras.metrics.CategoricalAccuracy('source_train_accuracy')\n",
        "target_train_accuracy = tf.keras.metrics.CategoricalAccuracy('target_train_accuracy')\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step_encoder(encoder_s,\n",
        "                       encoder_t,\n",
        "                       classifier,\n",
        "                       source_batch,\n",
        "                       target_batch,\n",
        "                       epoch):\n",
        "    \"\"\"encode the real samples,\n",
        "       make predictions of features,\n",
        "       =========BLOCK 1==========\"\"\"\n",
        "    # print(epoch)\n",
        "    source_data, source_label = get_data_from_batch(source_batch)\n",
        "    target_data, target_label = get_data_from_batch(target_batch)\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        source_feature = encoder_s(source_data, training=True)\n",
        "        target_feature = encoder_t(target_data, training=True)\n",
        "\n",
        "        classify_source = classifier(source_feature, training=True)\n",
        "        # print('source_decision', classify_source[0][:])\n",
        "        classify_loss = classifier_loss(classify_source, source_label)\n",
        "        encoder_s_loss = classifier_loss(classify_source, source_label)\n",
        "\n",
        "        classify_target = classifier(target_feature, training=True)\n",
        "        classify_loss += classifier_loss(classify_target, target_label)\n",
        "        encoder_t_loss = classifier_loss(classify_target, target_label)\n",
        "\n",
        "        gradient_source = tape.gradient(encoder_s_loss,\n",
        "                                        encoder_s.trainable_variables)\n",
        "        gradient_target = tape.gradient(encoder_t_loss,\n",
        "                                        encoder_t.trainable_variables)\n",
        "        gradient_classifier = tape.gradient(classify_loss,\n",
        "                                            classifier.trainable_variables)\n",
        "        encoder_s_optimizer.apply_gradients(zip(gradient_source,\n",
        "                                                encoder_s.trainable_variables))\n",
        "        encoder_t_optimizer.apply_gradients(zip(gradient_target,\n",
        "                                                encoder_t.trainable_variables))\n",
        "        classifier_optimizer.apply_gradients(zip(gradient_classifier,\n",
        "                                                 classifier.trainable_variables))\n",
        "    train_loss(classify_loss)\n",
        "    source_train_accuracy(source_label, classify_source)\n",
        "    target_train_accuracy(target_label, classify_target)\n",
        "    del tape\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step_domain(encoder_s,\n",
        "                      encoder_t,\n",
        "                      classifier,\n",
        "                      discriminator_domain,\n",
        "                      source_batch,\n",
        "                      target_batch):\n",
        "    \"\"\"encode the real samples,\n",
        "       discriminate domain,\n",
        "       =========BLOCK 2==========\"\"\"\n",
        "    source_data, source_label = get_data_from_batch(source_batch)\n",
        "    target_data, target_label = get_data_from_batch(target_batch)\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        source_feature = encoder_s(source_data, training=True)\n",
        "        target_feature = encoder_t(target_data, training=True)\n",
        "\n",
        "        disc_decision_s = discriminator_domain(source_feature, training=True)\n",
        "        disc_decision_t = discriminator_domain(target_feature, training=True)\n",
        "        prediction_s = classifier(source_feature, training=True)\n",
        "        prediction_t = classifier(target_feature, training=True)\n",
        "\n",
        "        \"\"\"if decision>0.5, then see it as source\n",
        "            otherwise see it as target,\n",
        "            0.5 means the discriminator cannot discriminate domains\"\"\"\n",
        "        encoder_s_loss = binary_cross_entropy(tf.zeros_like(disc_decision_s), disc_decision_s)\n",
        "        encoder_s_loss += classifier_loss(prediction_s, source_label)\n",
        "        encoder_t_loss = binary_cross_entropy(tf.ones_like(disc_decision_t), disc_decision_t)\n",
        "        encoder_t_loss += classifier_loss(prediction_t, target_label)\n",
        "        disc_domain_loss = discriminator_loss(disc_decision_s, disc_decision_t)\n",
        "\n",
        "        encoder_s_gradient = tape.gradient(encoder_s_loss, encoder_s.trainable_variables)\n",
        "        encoder_t_gradient = tape.gradient(encoder_t_loss, encoder_t.trainable_variables)\n",
        "        discriminator_domain_gradient = tape.gradient(disc_domain_loss, discriminator_domain.trainable_variables)\n",
        "\n",
        "        encoder_s_optimizer.apply_gradients(zip(encoder_s_gradient, encoder_s.trainable_variables))\n",
        "        encoder_t_optimizer.apply_gradients(zip(encoder_t_gradient, encoder_s.trainable_variables))\n",
        "        discriminator_domain_optimizer.apply_gradients(zip(discriminator_domain_gradient,\n",
        "                                                           discriminator_domain.trainable_variables))\n",
        "    train_loss(disc_domain_loss)\n",
        "    del tape\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step_double_GAN(generator_s, generator_t,\n",
        "                          discriminator_t, discriminator_s,\n",
        "                          batch_source, batch_target):\n",
        "    \"\"\"GAN block, generate target from source,\n",
        "       generate source from target\n",
        "       =========BLOCK 3==========\"\"\"\n",
        "    data_source, label_source = get_data_from_batch(batch_source)\n",
        "    data_target, label_target = get_data_from_batch(batch_target)\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        generated_t = generator_s(data_source, training=True)\n",
        "        generated_s = generator_t(data_target, training=True)\n",
        "        fake_decision_t = discriminator_t(generated_t)\n",
        "        real_decision_t = discriminator_t(data_target)\n",
        "        fake_decision_s = discriminator_s(generated_s)\n",
        "        real_decision_s = discriminator_s(data_source)\n",
        "\n",
        "        gen_s_loss = generator_loss(fake_decision_t)\n",
        "        disc_t_loss = discriminator_loss(real_decision_t, fake_decision_t)\n",
        "        gen_t_loss = generator_loss(fake_decision_s)\n",
        "        disc_s_loss = discriminator_loss(real_decision_s, fake_decision_s)\n",
        "\n",
        "        gradient_gen_s = tape.gradient(gen_s_loss, generator_s.trainable_variables)\n",
        "        gradient_gen_t = tape.gradient(gen_t_loss, generator_t.trainable_variables)\n",
        "        gradient_disc_t = tape.gradient(disc_t_loss, discriminator_t.trainable_variables)\n",
        "        gradient_disc_s = tape.gradient(disc_s_loss, discriminator_s.trainable_variables)\n",
        "\n",
        "        generator_s_optimizer.apply_gradients(zip(gradient_gen_s, generator_s.trainable_variables))\n",
        "        generator_t_optimizer.apply_gradients(zip(gradient_gen_t, generator_t.trainable_variables))\n",
        "        discriminator_t_optimizer.apply_gradients(zip(gradient_disc_t, discriminator_t.trainable_variables))\n",
        "        discriminator_s_optimizer.apply_gradients(zip(gradient_disc_s, discriminator_s.trainable_variables))\n",
        "    generator_train_loss(gen_s_loss)\n",
        "    discriminator_train_loss(disc_t_loss)\n",
        "    del tape\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step_whole(generator_s, generator_t,\n",
        "                     discriminator_t, discriminator_s,\n",
        "                     discriminator_domain,\n",
        "                     encoder_s, encoder_t,\n",
        "                     classifier,\n",
        "                     source_batch, target_batch\n",
        "                     ):\n",
        "    \"\"\"The whole training procedure,\n",
        "       including GAN_st, GAN_domain,\n",
        "          encode and classify\n",
        "       =========BLOCK 4==========\"\"\"\n",
        "    data_source, label_source = get_data_from_batch(source_batch)\n",
        "    data_target, label_target = get_data_from_batch(target_batch)\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # GAN source->target\n",
        "        generated_target = generator_s(data_source, training=True)\n",
        "        real_decision = discriminator_t(data_target, training=True)\n",
        "        fake_decision = discriminator_t(generated_target, training=True)\n",
        "        gen_s_loss = generator_loss(fake_decision)\n",
        "        disc_t_loss = discriminator_loss(real_decision, fake_decision)\n",
        "        gen_s_gradient = tape.gradient(gen_s_loss, generator_s.trainable_variables)\n",
        "        disc_t_gradient = tape.gradient(disc_t_loss, discriminator_t.trainable_variables)\n",
        "        generator_s_optimizer.apply_gradients(zip(gen_s_gradient, generator_s.trainable_variables))\n",
        "        discriminator_t_optimizer.apply_gradients(zip(disc_t_gradient, discriminator_t.trainable_variables))\n",
        "\n",
        "        # GAN target->source\n",
        "        generated_source = generator_t(data_target, training=True)\n",
        "        real_decision2 = discriminator_s(data_source, training=True)\n",
        "        fake_decision2 = discriminator_s(generated_source, training=True)\n",
        "        gen_t_loss = generator_loss(fake_decision2)\n",
        "        disc_s_loss = discriminator_loss(real_decision2, fake_decision2)\n",
        "        gen_t_gradient = tape.gradient(gen_t_loss, generator_t.trainable_variables)\n",
        "        disc_s_gradient = tape.gradient(disc_s_loss, discriminator_s.trainable_variables)\n",
        "        generator_t_optimizer.apply_gradients(zip(gen_t_gradient, generator_t.trainable_variables))\n",
        "        discriminator_s_optimizer.apply_gradients(zip(disc_s_gradient, discriminator_s.trainable_variables))\n",
        "\n",
        "        # GAN domain\n",
        "        feature_s = encoder_s(generated_source, training=True)\n",
        "        feature_t = encoder_t(generated_target, training=True)\n",
        "        feature_t_real = encoder_t(data_target, training=True)\n",
        "        target_decision = discriminator_domain(feature_t, training=True)\n",
        "        source_decision = discriminator_domain(feature_s, training=True)\n",
        "        disc_domain_loss = discriminator_loss(source_decision, target_decision)\n",
        "        encoder_s_loss = binary_cross_entropy(tf.zeros_like(source_decision), source_decision)\n",
        "        encoder_t_loss = binary_cross_entropy(tf.ones_like(target_decision), target_decision)\n",
        "        # calculate the gradient of encoders later, since it need to be combined with classification loss\n",
        "        disc_domain_gradient = tape.gradient(disc_domain_loss, discriminator_domain.trainable_variables)\n",
        "        discriminator_domain_optimizer.apply_gradients(zip(disc_domain_gradient,\n",
        "                                                           discriminator_domain.trainable_variables))\n",
        "        # classify\n",
        "        prediction_s = classifier(feature_s, training=True)\n",
        "        prediction_t = classifier(feature_t, training=True)\n",
        "        prediction_t_real = classifier(feature_t_real, training=True)\n",
        "        pred_loss = classifier_loss(prediction_t, label_source)\n",
        "        pred_loss += classifier_loss(prediction_t_real, label_target)\n",
        "        encoder_s_loss += classifier_loss(prediction_s, label_target)\n",
        "        encoder_t_loss += classifier_loss(prediction_t, label_source)\n",
        "        encoder_s_gradient = tape.gradient(encoder_s_loss, encoder_s.trainable_variables)\n",
        "        encoder_t_gradient = tape.gradient(encoder_t_loss, encoder_t.trainable_variables)\n",
        "        classifier_gradient = tape.gradient(pred_loss, classifier.trainable_variables)\n",
        "        encoder_s_optimizer.apply_gradients(zip(encoder_s_gradient, encoder_s.trainable_variables))\n",
        "        encoder_t_optimizer.apply_gradients(zip(encoder_t_gradient, encoder_t.trainable_variables))\n",
        "        classifier_optimizer.apply_gradients(zip(classifier_gradient, classifier.trainable_variables))\n",
        "    train_loss(pred_loss/2)\n",
        "    target_train_accuracy(label_source, prediction_t)\n",
        "    del tape\n",
        "\n",
        "\n",
        "def train(generator_s, generator_t,\n",
        "          discriminator_t, discriminator_s, discriminator_domain,\n",
        "          encoder_s, encoder_t, classifier,\n",
        "          source_train_ds, target_train_ds, source_test_ds, target_test_ds,\n",
        "          epochs):\n",
        "    whole_start = time.time()\n",
        "    # block1: encode and classify\n",
        "    patience = PATIENCE\n",
        "    wait = 0\n",
        "    best = 0\n",
        "    for epoch in range(100):\n",
        "        print('=====================- block 1 -========================')\n",
        "        start = time.time()\n",
        "        for source_batch in source_train_ds.as_numpy_iterator():\n",
        "            for target_batch in target_train_ds.as_numpy_iterator():\n",
        "                train_step_encoder(encoder_s,\n",
        "                                   encoder_t,\n",
        "                                   classifier,\n",
        "                                   source_batch,\n",
        "                                   target_batch, epoch)\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('block1_train_loss', train_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('block1_train_acc', target_train_accuracy.result(), step=epoch)\n",
        "        template = 'Epoch {}: Train loss={:.2f},' \\\n",
        "                   ' source_train_accuracy={:.2f}%,' \\\n",
        "                   ' target_train_accuracy={:.2f}%'\n",
        "        print(template.format(epoch+1,\n",
        "                              train_loss.result(),\n",
        "                              source_train_accuracy.result()*100,\n",
        "                              target_train_accuracy.result()*100))\n",
        "\n",
        "        for source_batch in source_test_ds.as_numpy_iterator():\n",
        "            for target_batch in target_test_ds.as_numpy_iterator():\n",
        "                test_step1(encoder_s, encoder_t, classifier, source_batch, target_batch)\n",
        "        template = 'Epoch {}: Test loss={:.2f}, ' \\\n",
        "                   'source_test_accuracy={:.2f}%,' \\\n",
        "                   ' target_test_accuracy={:.2f}%'\n",
        "        print(template.format(epoch + 1, test_loss.result(),\n",
        "                              source_test_accuracy.result() * 100,\n",
        "                              target_test_accuracy.result() * 100))\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar('block1_test_loss', test_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('block1_test_target_accuracy', target_test_accuracy.result(), step=epoch)\n",
        "        test_acc = target_test_accuracy.result().numpy()\n",
        "        # Reset metrics every epoch\n",
        "        train_loss.reset_states()\n",
        "        test_loss.reset_states()\n",
        "        source_train_accuracy.reset_states()\n",
        "        target_train_accuracy.reset_states()\n",
        "        source_test_accuracy.reset_states()\n",
        "        target_test_accuracy.reset_states()\n",
        "        print('Time for epoch{} is {:.2f} sec'.format(epoch+1, time.time()-start))\n",
        "        if epoch > 30:\n",
        "            wait += 1\n",
        "            if test_acc > best:\n",
        "                best = test_acc\n",
        "                wait = 0\n",
        "            if wait >= patience:\n",
        "                break\n",
        "    print('Saving encoder and classifier...')\n",
        "    tf.saved_model.save(encoder_s, './Model/block1/encoder_s')\n",
        "    tf.saved_model.save(encoder_t, './Model/block1/encoder_t')\n",
        "    tf.saved_model.save(classifier, './Model/block1/classifier')\n",
        "\n",
        "    # block2: encode, GAN_domain and classify\n",
        "    for epoch in range(100):\n",
        "        print('=====================- block 2 -========================')\n",
        "        start = time.time()\n",
        "        for source_batch in source_train_ds.as_numpy_iterator():\n",
        "            for target_batch in target_train_ds.as_numpy_iterator():\n",
        "                train_step_domain(encoder_s,\n",
        "                                  encoder_t,\n",
        "                                  classifier,\n",
        "                                  discriminator_domain,\n",
        "                                  source_batch,\n",
        "                                  target_batch)\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('block2_train_disc_domain_loss', train_loss.result(), step=epoch)\n",
        "        template = 'Train epoch {}, discriminator_domain_loss={:.2f}'\n",
        "        print(template.format(epoch+1, train_loss.result().numpy()))\n",
        "        for source_batch in source_test_ds.as_numpy_iterator():\n",
        "            for target_batch in target_test_ds.as_numpy_iterator():\n",
        "                test_step2(encoder_s, encoder_t, discriminator_domain, source_batch, target_batch)\n",
        "        template = 'Test epoch {}, discriminator_source_loss={:.2f}, discriminator_target_loss={:.2f}'\n",
        "        print(template.format(epoch+1, domain_source_loss.result().numpy(), domain_target_loss.result().numpy()))\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar('block2_test_domain_target_loss', domain_target_loss.result(), step=epoch)\n",
        "        train_loss.reset_states()\n",
        "        domain_source_loss.reset_states()\n",
        "        domain_target_loss.reset_states()\n",
        "        print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))\n",
        "    print('Saving encoder, discriminator_domain and classifier...')\n",
        "    tf.saved_model.save(encoder_s, './Model/block2/encoder_s')\n",
        "    tf.saved_model.save(encoder_t, './Model/block2/encoder_t')\n",
        "    tf.saved_model.save(discriminator_domain, './Model/block2/discriminator_domain')\n",
        "    tf.saved_model.save(classifier, './Model/block2/classifier')\n",
        "\n",
        "    # block3: GAN_transfer\n",
        "    for epoch in range(100):\n",
        "        print('=====================- block 3 -========================')\n",
        "        start = time.time()\n",
        "        for source_batch in source_train_ds.as_numpy_iterator():\n",
        "            for target_batch in target_train_ds.as_numpy_iterator():\n",
        "                train_step_double_GAN(generator_s,\n",
        "                                      generator_t,\n",
        "                                      discriminator_t,\n",
        "                                      discriminator_s,\n",
        "                                      source_batch,\n",
        "                                      target_batch)\n",
        "        template = 'Train epoch {}, generator_loss={:.2f}, discriminator_loss={:.2f}'\n",
        "        print(template.format(epoch+1, generator_train_loss.result().numpy(),\n",
        "                              discriminator_train_loss.result().numpy()))\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('block3_train_gen_loss', generator_train_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('block3_train_disc_loss', discriminator_train_loss.result(), step=epoch)\n",
        "        for source_batch in source_test_ds.as_numpy_iterator():\n",
        "            for target_batch in target_test_ds.as_numpy_iterator():\n",
        "                test_step3(generator_s, generator_t, discriminator_t, discriminator_s,\n",
        "                           source_batch, target_batch)\n",
        "        template = 'Test epoch {}, generator_loss={:.2f}, discriminator_loss={:.2f}'\n",
        "        print(template.format(epoch+1, gen_loss.result(), disc_loss.result()))\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar('block3_test_gen_loss', gen_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('block3_test_disc_loss', disc_loss.result(), step=epoch)\n",
        "        if epoch % 15 == 0:\n",
        "            generate_and_save_Images(generator_s, epoch, source_test_ds.as_numpy_iterator().next()['data'])\n",
        "        print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))\n",
        "        generator_train_loss.reset_states()\n",
        "        discriminator_train_loss.reset_states()\n",
        "        gen_loss.reset_states()\n",
        "        disc_loss.reset_states()\n",
        "    print('Saving generator and discriminator...')\n",
        "    tf.saved_model.save(generator_s, './Model/block3/generator_s')\n",
        "    tf.saved_model.save(generator_t, './Model/block3/generator_t')\n",
        "    tf.saved_model.save(discriminator_t, './Model/block3/discriminator_t')\n",
        "    tf.saved_model.save(discriminator_s, './Model/block3/discriminator_s')\n",
        "\n",
        "    # block4 the whole network\n",
        "    patience = PATIENCE\n",
        "    wait = 0\n",
        "    best = 0\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        train_acc = 0\n",
        "        print('=====================- block 4 -========================')\n",
        "        for source_batch in source_train_ds.as_numpy_iterator():\n",
        "            for target_batch in target_train_ds.as_numpy_iterator():\n",
        "                train_step_whole(generator_s, generator_t,\n",
        "                                 discriminator_t, discriminator_s,\n",
        "                                 discriminator_domain,\n",
        "                                 encoder_s, encoder_t,\n",
        "                                 classifier,\n",
        "                                 source_batch, target_batch)\n",
        "        template = 'Train epoch {}, classifier_loss={:.2f}, classifier_accuracy={:.2f}%'\n",
        "        print(template.format(epoch+1,\n",
        "                              train_loss.result(),\n",
        "                              target_train_accuracy.result()*100))\n",
        "        train_acc = target_train_accuracy.result().numpy()\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('block4_loss', train_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('block4_acc', target_train_accuracy.result(), step=epoch)\n",
        "        for source_batch in source_test_ds.as_numpy_iterator():\n",
        "            test_step4(generator_s, encoder_t, classifier, source_batch)\n",
        "        template = 'Test epoch {}, classifier_loss={:.2f}, classifier_accuracy={:.2f}%'\n",
        "        print(template.format(epoch+1,\n",
        "                              test_loss.result(),\n",
        "                              target_test_accuracy.result()*100))\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar('blcok4_loss', test_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('block_acc', target_test_accuracy.result(), step=epoch)\n",
        "        if epoch % 15 == 0:\n",
        "            generate_and_save_Images(generator_s, epoch, source_test_ds.as_numpy_iterator().next()['data'])\n",
        "        train_loss.reset_states()\n",
        "        target_train_accuracy.reset_states()\n",
        "        test_loss.reset_states()\n",
        "        target_test_accuracy.reset_states()\n",
        "        print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))\n",
        "        if epoch >= 700:\n",
        "            wait += 1\n",
        "            if train_acc > best:\n",
        "                best = train_acc\n",
        "                wait = 0\n",
        "            if wait >= patience:\n",
        "                break\n",
        "    print('Saving whole network...')\n",
        "    tf.saved_model.save(generator_s, './Model/block4/generator_s')\n",
        "    tf.saved_model.save(generator_t, './Model/block4/generator_t')\n",
        "    tf.saved_model.save(discriminator_t, './Model/block4/discriminator_t')\n",
        "    tf.saved_model.save(discriminator_s, './Model/block4/discriminator_s')\n",
        "    tf.saved_model.save(encoder_s, './Model/block4/encoder_s')\n",
        "    tf.saved_model.save(encoder_t, './Model/block4/encoder_t')\n",
        "    tf.saved_model.save(discriminator_domain, './Model/block4/discriminator_domain')\n",
        "    tf.saved_model.save(classifier, './Model/block4/classifier')\n",
        "    print('The whole procedure takes {} sec'.format(time.time()-whole_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjdTdXp3PGub",
        "outputId": "57801478-c3a7-4931-bafd-2d17330c305d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================- block 1 -========================\n"
          ]
        }
      ],
      "source": [
        "generator_s = make_generator_model()\n",
        "generator_t = make_generator_model()\n",
        "discriminator_t = make_discriminator_model()\n",
        "discriminator_s = make_discriminator_model()\n",
        "discriminator_domain = make_discriminator_domain_model()\n",
        "encoder_s = make_encoder_model()\n",
        "encoder_t = make_encoder_model()\n",
        "classifier = make_classifier_model()\n",
        "\n",
        "train(generator_s, generator_t,\n",
        "      discriminator_t, discriminator_s,\n",
        "      discriminator_domain,\n",
        "      encoder_s, encoder_t, classifier,\n",
        "      source_train_ds, target_train_ds,\n",
        "      source_test_ds, target_test_ds,\n",
        "      EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
        "val_accuracy = tf.keras.metrics.CategoricalAccuracy('val_accuracy')\n",
        "\n",
        "for val_batch in target_val_ds.as_numpy_iterator():\n",
        "    x, y = get_data_from_batch(val_batch)\n",
        "    x_feature = encoder_t(x, training=False)\n",
        "    prediction = classifier(x_feature, training=False)\n",
        "    val_loss(classifier_loss(prediction, y))\n",
        "    val_accuracy(y, prediction)\n",
        "    template = 'Loss: {}, Accuracy: {}'\n",
        "    print(template.format(val_loss.result(),\n",
        "                          val_accuracy.result() * 100))\n"
      ],
      "metadata": {
        "id": "OJkwCxGjE3Qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "doublebigan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}